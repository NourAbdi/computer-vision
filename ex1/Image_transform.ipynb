{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a5c816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the images\n",
    "image_filenames=[]\n",
    "for i in range(1,12):\n",
    "    img =  f\"C:/Users/user/Downloads/ex1/puzzles/puzzles/puzzle_affine_4/pieces/piece_{i}.jpg\"\n",
    "    image_filenames.append(img)\n",
    "images = [cv2.imread(filename) for filename in image_filenames]\n",
    "\n",
    "'''\n",
    "Here's how Load the images works:\n",
    "\n",
    "A list called image_filenames is created and initialized as an empty list.\n",
    "\n",
    "A loop is run 36 times (because we have 36 images inside the pieces folder), with i taking on the values from 1 to 36.\n",
    "\n",
    "Inside the loop, a string called img is created using an f-string. The f-string contains the path to an image file, with {i} used as a placeholder for the value of the loop variable.\n",
    "\n",
    "The string img is appended to the list image_filenames.\n",
    "\n",
    "After the loop completes, a list comprehension is used to load each image file in image_filenames using the cv2.imread function. The loaded images are stored in a list called images.\n",
    "\n",
    "So, at the end of this code, images is a list of 36 images loaded from the image files specified in image_filenames.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Create a SIFT object \n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for each image\n",
    "keypoints_list = []\n",
    "descriptors_list = []\n",
    "for image in images:\n",
    "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "    keypoints_list.append(keypoints)\n",
    "    descriptors_list.append(descriptors)\n",
    "\n",
    "# Create a feature matcher\n",
    "matcher = cv2.FlannBasedMatcher_create()\n",
    "\n",
    "# Match keypoints and descriptors between adjacent images with ratio test\n",
    "matches_list = []\n",
    "for i in range(len(images)-1):\n",
    "    matches = matcher.knnMatch(descriptors_list[i], descriptors_list[i+1], k=2)\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.99 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    matches_list.append(good_matches)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Match keypoints and descriptors between adjacent images with ratio test\n",
    "\n",
    "# Apply RANSAC to estimate the affine transformation between matched keypoints\n",
    "homography_list = []\n",
    "for matches, keypoints1, keypoints2 in zip(matches_list, keypoints_list[:-1], keypoints_list[1:]):\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    M, mask = cv2.estimateAffine2D(src_pts, dst_pts, ransacReprojThreshold=5.0)\n",
    "    homography_list.append(M)\n",
    "\n",
    "# Warp the images to align with the first image\n",
    "height, width = images[0].shape[:2]\n",
    "warped_images = []\n",
    "for i in range(len(images)):\n",
    "    if i == 0:\n",
    "        warped_images.append(images[i])\n",
    "    else:\n",
    "        warped_img = cv2.warpAffine(images[i], homography_list[i-1], (width*(i+1), height))\n",
    "        warped_images.append(warped_img)\n",
    "\n",
    "# Combine the images into a panorama\n",
    "panorama = np.zeros((height, width*(len(images)+1), 3), dtype=np.uint8)\n",
    "panorama[:, :width] = warped_images[0]\n",
    "for i in range(1, len(warped_images)):\n",
    "    panorama[:, i*width:(i+1)*width] = warped_images[i][:, :width]\n",
    "#cv2.imwrite('panor_affine.jpg', panorama)\n",
    "# Display the panorama\n",
    "plt.imshow(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "match_conf=500\n",
    "stitcher = cv2.Stitcher_create()\n",
    "#stitcher.setMatchConf(0.5)\n",
    "(status, stitched) = stitcher.stitch(images)\n",
    "\n",
    "# Display the stitched image\n",
    "if status == cv2.STITCHER_OK:\n",
    "    # Display the stitched image\n",
    "    plt.imshow(cv2.cvtColor(stitched, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    cv2.imwrite('orama_planaasr_1.jpg', stitched)\n",
    "else:\n",
    "    # Handle the case where stitching fails\n",
    "    print('Error: Failed to stitch images')\n",
    "\n",
    "# Save the stitched image\n",
    "\n",
    "\n",
    "#SAVING THE IMAGE IN A SEPARATE FOLDER\n",
    "for i in range(len(warped_images)):\n",
    "    x_start = i * width\n",
    "    x_end = (i + 1) * width\n",
    "    y_start = 0\n",
    "    y_end = height\n",
    "    region = panorama[y_start:y_end, x_start:x_end]\n",
    "    cv2.imwrite(f'C:/Users/user/Downloads/ex1/puzzles/puzzles/puzzle_affine_1/piece/puzzle_piece_{i+1}.jpg', region)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
